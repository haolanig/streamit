{
 "cells": [
  {
   "cell_type": "raw",
   "id": "a5a447bb",
   "metadata": {},
   "source": [
    "Nous travaillons pour une société financière nommait \"Prêt à dépenser\" qui propose des crédits à la consommation pour des personnes ayant peu ou pas du tout d'historique de prêt. L’entreprise souhaite mettre en œuvre un outil de “scoring crédit” pour calculer la probabilité qu’un client rembourse son crédit, puis classifie la demande en crédit accordé ou refusé. Il faudra développer un algorithme de classification en s'appuyant sur des sources de données variées (données comportementales, données provenant d'autres institutions financières, etc.).\n",
    "Les clients sont de plus en plus demandeurs de transparence vis-à-vis des décisions d’octroi de crédit.\n",
    "Prêt à dépenser décide donc de développer un dashboard interactif pour que les chargés de relation client puissent à la fois expliquer de façon la plus transparente possible les décisions d’octroi de crédit, mais également permettre à leurs clients de disposer de leurs informations personnelles et de les explorer facilement.\n",
    "\n",
    "Notre mission : - Construire un modèle de scoring qui donnera une prédiction sur la probabilité de faillite d'un client de façon automatique. - Construire un dashboard interactif à destination des gestionnaires de la relation client permettant d'interpréter les prédictions faites par le modèle, et d’améliorer la connaissance client des chargés de relation client.\n",
    "\n",
    "Spécifications du dashboard\n",
    "Michaël vous a fourni des spécifications pour le dashboard interactif. Celui-ci devra contenir au minimum les fonctionnalités suivantes :\n",
    "\n",
    "    Permettre de visualiser le score et l’interprétation de ce score pour chaque client de façon intelligible pour une personne non experte en data science.\n",
    "    Permettre de visualiser des informations descriptives relatives à un client (via un système de filtre).\n",
    "    Permettre de comparer les informations descriptives relatives à un client à l’ensemble des clients ou à un groupe de clients similaires.\n",
    "\n",
    "Sommaire :\n",
    "\n",
    "    Importation des librairies de bases et des dataset\n",
    "    Analyse exploratoire\n",
    "    Gestion des valeurs manquantes\n",
    "        Les types 'object'\n",
    "        Les types 'float64'\n",
    "    Gestion des outliers\n",
    "    Suppression des doublons"
   ]
  },
  {
   "cell_type": "raw",
   "id": "b9909370",
   "metadata": {},
   "source": [
    "A faire :\n",
    "\n",
    "afficher la matrice de confusion directement en sortie de pipeline ?\n",
    "\n",
    "tester un SVM avec kernel linéaire puis gaussien('rbf')/!\\ celui par défaut est le rbf (SVC.decision_function)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74864bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "#pd.set_option('display.max_colwidth', 200, 'display.max_rows', None, 'display.max_columns', None)\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import plotly.express as px\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import make_column_transformer\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.tree import DecisionTreeClassifier, plot_tree\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import plot_confusion_matrix, fbeta_score, make_scorer, accuracy_score\n",
    "from sklearn.dummy import DummyClassifier\n",
    "from sklearn import set_config\n",
    "import glob\n",
    "#set_config(display='diagram')\n",
    "#print('Imports terminés.')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "588ac4c6",
   "metadata": {},
   "source": [
    "# Importation des données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bba635fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "#os.chdir('C:\\\\Users\\\\hlouk\\\\OpenClassroom\\\\P7')\n",
    "os.chdir('C:/Users/Allihamdulilahi/Desktop/invest_ideas/han/p7')\n",
    "print(os.getcwd())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e14ff53",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"CHECKLIST DATA EXPLORER :\")\n",
    "#display(os.listdir('C:\\\\Users\\\\hlouk\\\\OpenClassroom\\\\P7'))\n",
    "display(os.listdir('C:/Users/Allihamdulilahi/Desktop/invest_ideas/han/p7'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abf0ddc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "missing_values = [\"n/a\", \"na\", \"--\",\"nan\",\"NaN\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d201b5b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time \n",
    "\n",
    "#dirpath = \"C:/Users/hlouk/OpenClassroom/P7/\"\n",
    "dirpath = \"C:/Users/Allihamdulilahi/Desktop/invest_ideas/han/p7/\"\n",
    "\n",
    "application_train = pd.read_csv(dirpath+\"application_train.csv\")\n",
    "application_test = pd.read_csv(dirpath+\"application_test.csv\")\n",
    "bureau = pd.read_csv(dirpath+\"bureau.csv\")\n",
    "bureau_balance = pd.read_csv(dirpath+\"bureau_balance.csv\")\n",
    "cash_balance = pd.read_csv(dirpath+\"POS_CASH_balance.csv\")\n",
    "card_balance = pd.read_csv(dirpath+\"credit_card_balance.csv\")\n",
    "prev_app = pd.read_csv(dirpath+\"previous_application.csv\")\n",
    "payments = pd.read_csv(dirpath+\"installments_payments.csv\")\n",
    "HomeCredit = pd.read_csv(dirpath+\"HomeCredit_columns_description.csv\",  na_values = missing_values, encoding='latin1')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a919a831",
   "metadata": {},
   "source": [
    "Les données sont réparties en 9 fichiers.\n",
    "\n",
    "  1 fichier principal pour la partie \"training\" importé dans train_set (1 ligne / prêt)\n",
    "  1 fichier principal pour la partie \"test\" importé dans test_set (1 ligne / prêt)\n",
    "  1 fichier HomeCredit_columns_description propose le descriptif de chaque colonne dans chaque fichier\n",
    "  6 autres fichiers contenant des informations supplémentaires sur chaque prêt.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f9f58b0",
   "metadata": {},
   "source": [
    "# Analyse et pré-traitement"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6af560b8",
   "metadata": {},
   "source": [
    "## Train/Test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4bf36151",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training data\n",
    "print('Training data shape: ', application_train.shape)\n",
    "application_train.describe()\n",
    "application_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3c060a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Testing data\n",
    "print('application_test shape: ', application_test.shape)\n",
    "application_test.describe()\n",
    "application_test.head()\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9df91679",
   "metadata": {},
   "source": [
    "Le train_set est 6 fois plus imposant que le test_set et a une colone TARGET en plus qu'on tentera de prédire pour le test_set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8791ed3c",
   "metadata": {},
   "source": [
    "## Autres Fichiers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bde0f4ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bureau\n",
    "print('Bureau data shape: ', bureau.shape)\n",
    "#print(bureau.describe())\n",
    "bureau.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b33ade51",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Bureau balance\n",
    "print('Bureau balance data shape: ', bureau_balance.shape)\n",
    "#print(bureau_balance.describe())\n",
    "bureau_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d25b1951",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cash balance\n",
    "print('Cash balance data shape: ', cash_balance.shape)\n",
    "#print(cash_balance.describe())\n",
    "cash_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e71765c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Card balance\n",
    "print('Card balance data shape: ', card_balance.shape)\n",
    "#print(card_balance.describe())\n",
    "card_balance.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8753cdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Previous applications balance\n",
    "print('Previous applications data shape: ', prev_app.shape)\n",
    "#print(prev_app.describe())\n",
    "prev_app.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05ebaa86",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Payments balance\n",
    "print('Payments data shape: ', payments.shape)\n",
    "#print(payments.describe())\n",
    "payments.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fdd95d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# HomeCredit balance\n",
    "print('HomeCredit data shape: ', HomeCredit.shape)\n",
    "#print(HomeCredit.describe())\n",
    "HomeCredit.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40d0de5e",
   "metadata": {},
   "source": [
    "# Analyse des données"
   ]
  },
  {
   "cell_type": "raw",
   "id": "3de3db2f",
   "metadata": {},
   "source": [
    "Le tableau HomeCredit_columns_description propose le descriptif de chaque colonne dans chaque fichier. \n",
    "Commençons par créer une fonction qui permet de consulter ce descriptif pour une table donnée, ainsi qu'une fonction qui permet d'avoir un aperçu général d'une table donnée."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "777b5b08",
   "metadata": {},
   "outputs": [],
   "source": [
    "#columns_description=pd.read_csv(\"C:\\\\Users\\\\hlouk\\\\OpenClassroom\\\\P7\\\\HomeCredit_columns_description.csv\",encoding='latin1')\n",
    "columns_description=pd.read_csv(\"C:\\\\Users\\\\Allihamdulilahi\\\\Desktop\\\\invest_ideas\\\\han\\\\p7\\\\HomeCredit_columns_description.csv\",encoding='latin1')\n",
    "\n",
    "\n",
    "def describe_sheet(sheet_name):\n",
    "    '''Display content of HomeCredit_columns_description for sheet_name table.'''\n",
    "    if 'application' in sheet_name:\n",
    "        sheet_name = dirpath + 'application_{train|test}'\n",
    "    description = columns_description[columns_description['Table'].str.contains(sheet_name)].iloc[:,2:]\n",
    "    description = description.set_index('Row')\n",
    "    return description\n",
    "        \n",
    "\n",
    "def overview(df_name):\n",
    "    '''Display information, result of describe, shape and head for given df_name.'''\n",
    "    df = globals()[df_name]\n",
    "    print('\\n\\n' + '#'*60)\n",
    "    print(f'Table {df_name}')\n",
    "    print('#'*60)\n",
    "    print(f'\\n\\nLa table {df_name} a {df.shape[0]:,.0f} lignes et {df.shape[1]:.0f} colonnes.'.replace(',', ' '))\n",
    "    print(f'\\n\\nDescription de la table :\\n')\n",
    "    display(describe_sheet(df_name))\n",
    "    print('\\n\\nPrincipales statistiques :\\n')\n",
    "    display(df.describe(include='all'))\n",
    "    print(f'\\n\\nPremières lignes :\\n')\n",
    "    display(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5603b9cb",
   "metadata": {},
   "source": [
    "## Apercu des Données"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "836e1d2e",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train = pd.read_csv(dirpath + 'application_train.csv')\n",
    "\n",
    "overview('application_train')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2bfa37b",
   "metadata": {},
   "source": [
    "## Les valeurs manquantes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63223cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = application_train.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eba65f83",
   "metadata": {},
   "outputs": [],
   "source": [
    "def data_describe(folder):\n",
    "    '''Check the number of rows, columns, missing values and duplicates.\n",
    "       Count type of columns.\n",
    "       Memory indication'''\n",
    "\n",
    "    data_dict = {}\n",
    "    for file in folder:\n",
    "        data = pd.read_csv(file)\n",
    "        data_dict[file] = [data.shape[0], \n",
    "                           data.shape[1],\n",
    "                            round(data.isna().sum().sum()/data.size*100, 2),\n",
    "                            round(data.duplicated().sum().sum()/data.size*100, 2),\n",
    "                            data.select_dtypes(include=['object']).shape[1],\n",
    "                            data.select_dtypes(include=['float']).shape[1],\n",
    "                            data.select_dtypes(include=['int']).shape[1],\n",
    "                            data.select_dtypes(include=['bool']).shape[1],\n",
    "                            round(data.memory_usage().sum()/1024**2, 3)]\n",
    "\n",
    "        comparative_table = pd.DataFrame.from_dict(data = data_dict, \n",
    "                                                   columns = ['Rows', 'Columns', '%NaN', '%Duplicate', \n",
    "                                                              'object_dtype','float_dtype', 'int_dtype', \n",
    "                                                              'bool_dtype', 'MB_Memory'], \n",
    "                                                   orient='index')\n",
    "    print(\"SUMMARY FILES…\")\n",
    "    return(comparative_table)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ace5b1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Data description  \n",
    "data_describe(folder = glob.glob((dirpath+\"application_test.csv\")))\n",
    "data_describe(folder = glob.glob((dirpath+\"bureau.csv\")))\n",
    "\n",
    "\n",
    "#,\" application_train.csv\", bureau.csv, bureau_balance.csv, credit_card_balance.csv, installments_payments.csv, POS_CASH_balance.csv,previous_application.csv, sample_submission.csv\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79efe9be",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction qui calcule les valeurs manquantes par colone\n",
    "def missing_values_table(df):\n",
    "    # Total missing values\n",
    "    mis_val = df.isnull().sum()\n",
    "        \n",
    "    # Percentage of missing values\n",
    "    mis_val_percent = 100 * df.isnull().sum() / len(df)\n",
    "        \n",
    "    # Make a table with the results\n",
    "    mis_val_table = pd.concat([mis_val, mis_val_percent], axis=1)\n",
    "        \n",
    "    # Rename the columns\n",
    "    mis_val_table_ren_columns = mis_val_table.rename(\n",
    "    columns = {0 : 'Missing Values', 1 : '% of Total Values'})\n",
    "        \n",
    "    # Sort the table by percentage of missing descending\n",
    "    mis_val_table_ren_columns = mis_val_table_ren_columns[mis_val_table_ren_columns.iloc[:,1] != 0].sort_values(\n",
    "        '% of Total Values', ascending=False).round(1)\n",
    "\n",
    "    # Print some summary information\n",
    "    print (\"Your selected dataframe has \" + str(df.shape[1]) + \" columns.\\n\"      \n",
    "           \"There are \" + str(mis_val_table_ren_columns.shape[0]) +\n",
    "           \" columns that have missing values.\")\n",
    "        \n",
    "    # Return the dataframe with missing information\n",
    "    return mis_val_table_ren_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a644293e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Statistiques des valeurs manquantes\n",
    "missing_values = missing_values_table(data)\n",
    "missing_values.head(20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4457374e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nombre de chaque type de colonne\n",
    "# float64 et int64 pour des variables numériques\n",
    "# object pour des variables catégorielles\n",
    "data.dtypes.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b115a95",
   "metadata": {},
   "source": [
    "## Les anomalies des valeurs numériques"
   ]
  },
  {
   "cell_type": "raw",
   "id": "56b0590d",
   "metadata": {},
   "source": [
    "les distributions pour les variables numériques."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "418ec569",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_hist_data(df):\n",
    "    num_cols = [col for col in df.columns if data[col].dtype != 'object'] \n",
    "        \n",
    "    height = int(np.ceil(len(num_cols)/6))\n",
    "    fig_height = 3 * height\n",
    "    fig = plt.figure(figsize=(20,fig_height))\n",
    "    \n",
    "    for feat_idx, col in enumerate(num_cols):\n",
    "        ax = fig.add_subplot(height, 6, feat_idx+1)\n",
    "        ax.hist(df[col], bins=50)\n",
    "        ax.set_title(col)\n",
    "    fig.tight_layout(pad=4)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d338473e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_box_data(df):\n",
    "    num_cols = [col for col in df.columns if data[col].dtype != 'object'] \n",
    "    \n",
    "    height = int(np.ceil(len(num_cols)/6))\n",
    "    fig_height = 3 * height\n",
    "    fig = plt.figure(figsize=(20,fig_height))\n",
    "    \n",
    "    for feat_idx, col in enumerate(num_cols):\n",
    "        ax = fig.add_subplot(height, 6, feat_idx+1)\n",
    "        ax.boxplot(df[col])\n",
    "        ax.set_title(col)\n",
    "    fig.tight_layout(pad=4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e5faabe",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "plot_hist_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6420342",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_box_data(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6903fcd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "px.box(data['AMT_INCOME_TOTAL'])"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9ca2600c",
   "metadata": {},
   "source": [
    "On observe des valeurs irréalistes de cette colonne\n",
    "\n",
    "Pour écarter les valeurs aberrantes, on utilisera le Z-score, c'est-à-dire la distance à la moyenne divisée par l'écart-type.\n",
    "\n",
    "Si Z-score > 3, la valeur peut être considérée comme un outlier puisqu'elle ne fait pas partie des 99,7% valeurs les plus proches de la moyenne."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db681628",
   "metadata": {},
   "outputs": [],
   "source": [
    "def z_score(array, threshold=3):\n",
    "    '''Return an array of boolean, True for each value in the array where Z-score >threshold.'''\n",
    "    mean = array.mean()\n",
    "    std = array.std()\n",
    "    return abs((array-mean))/std > threshold"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eadd08be",
   "metadata": {},
   "outputs": [],
   "source": [
    "nb_outliers = data[z_score(data['AMT_INCOME_TOTAL'])].shape[0]\n",
    "pc_outliers = nb_outliers / data.shape[0]\n",
    "print(f'Concernant la variable AMT_INCOME_TOTAL, on retrouve {nb_outliers} outliers \\\n",
    "qui représentent {pc_outliers:.2%} des cas.')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5b73a64f",
   "metadata": {},
   "source": [
    "Examinons les 10 plus gros revenus de la liste des emprunteurs :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e4a322",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.sort_values(by='AMT_INCOME_TOTAL', ascending=False).head(10).style.format({'AMT_INCOME_TOTAL':'{:,.0f}'})"
   ]
  },
  {
   "cell_type": "raw",
   "id": "9a7f8650",
   "metadata": {},
   "source": [
    "Puisque le model dois etre concu pour les gens aux revenues normaux, nous écarterons les données avec un Z-score supérieur à 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdfdea75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Convertissons-les temporairement en années écoulées (divison par -365) pour faciliter la vérification \n",
    "\n",
    "day_cols = [col for col in data.columns if \"DAYS_\" in col]\n",
    "(data[day_cols].describe().loc[['min', 'max']].transpose()/-365).style.format({'min':'{:,.2f}', 'max':'{:,.2f}'})\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf11caac",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data[day_cols])[data['DAYS_EMPLOYED']>0].describe()"
   ]
  },
  {
   "cell_type": "raw",
   "id": "286abff9",
   "metadata": {},
   "source": [
    "365243.0 est la valeur en commun entre plusieurs cases ce qui suggere une valeur aberrante "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "788d002f",
   "metadata": {},
   "outputs": [],
   "source": [
    "days_count_pos = data['DAYS_EMPLOYED'][data['DAYS_EMPLOYED']>0].count()\n",
    "print(f\"Nombre de lignes avec des valeurs aberrantes pour DAYS_EMPLOYED : {days_count_pos} soit {days_count_pos/data.shape[0]:.2%} du total\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88d4f539",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Affiche l'histogramme du nombre de jours employés\n",
    "fig = plt.figure(1, figsize=(12, 6))\n",
    "plt.subplot(1, 2, 1)\n",
    "data['DAYS_EMPLOYED'].hist()\n",
    "plt.xlabel('DAYS_EMPLOYED')\n",
    "plt.ylabel('Count')\n",
    "plt.xticks(rotation=30)\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5dddbf52",
   "metadata": {},
   "source": [
    "Nombre de cas concernés est trop important pour supprimer purement et simplement les lignes correspondantes. Nous allons prendre le parti de transformer ces valeurs en la valeur réaliste la plus proche, soit 0. Aux yeux d'un banquier, ne pas avoir d'emploi est à peu près équivalent à démarrer dans un poste le jour même de la demande de prêt."
   ]
  },
  {
   "cell_type": "raw",
   "id": "d151c30d",
   "metadata": {},
   "source": [
    "Les nombres de la colonne DAYS_BIRTH sont négatifs car ils sont enregistrés par rapport à la demande de prêt en cours. Il est nécessaire de modifier cette variable pour obtenir des chiffres plus compréhensibles pour l'analyse.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c61073d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# âge \n",
    "\n",
    "for col  in day_cols:\n",
    "    idx = data[data[col]<data['DAYS_BIRTH']].index\n",
    "    idx_exist = len(idx)\n",
    "    if idx_exist:\n",
    "        print(col, 'présente des valeurs aberrantes aux lignes :', list(idx))\n",
    "        for i in idx:\n",
    "            print(data.loc[i,[col, 'DAYS_BIRTH']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "90a6178f",
   "metadata": {},
   "outputs": [],
   "source": [
    "(data['DAYS_BIRTH'] / -365).describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "58fc4603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "plt.figure(figsize=(20, 5))\n",
    "sns.histplot((data['DAYS_BIRTH'] / -365), bins = 30).set_title('Age of client')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a02b57fd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b003924f",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[data['DAYS_EMPLOYED']>0].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffb6b7f4",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train['DAYS_EMPLOYED'].plot.hist(title = 'Days Employment Histogram', figsize=(20,5));\n",
    "plt.xlabel('Days Employment')"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f4e2a9ce",
   "metadata": {},
   "source": [
    " ces données ne sont pas \"normales\" au sens où le Max. représente 1000 années (365243/365j). Est-ce un individu isolé? Plusieurs individus de l'échantillon?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c902e38d",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"%0.0f values with 365243 days employed for training data\" % \n",
    "      len(application_train[application_train['DAYS_EMPLOYED'] == 365243]))\n",
    "print(\"%0.0f Total values from days employed for training data\" % application_train.shape[0])\n",
    "print(\"***********************\")\n",
    "print(\"%0.0f values with 365243 days employed for testing data\" % \n",
    "      len(application_test[application_test['DAYS_EMPLOYED'] == 365243]))\n",
    "print(\"%0.0f Total values from days employed for testing data\" % application_test.shape[0])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acec36fb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create an outliers flag column\n",
    "application_train['DAYS_EMPLOYED_OUTLIERS'] = application_train[\"DAYS_EMPLOYED\"] == 365243\n",
    "application_test['DAYS_EMPLOYED_OUTLIERS'] = application_test[\"DAYS_EMPLOYED\"] == 365243\n",
    "\n",
    "#Replace outliers values with nan\n",
    "application_train['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)\n",
    "application_test['DAYS_EMPLOYED'].replace({365243: np.nan}, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3196255",
   "metadata": {},
   "outputs": [],
   "source": [
    "application_train['DAYS_EMPLOYED'].plot.hist(title ='Days Employment with preprocessing outliers', figsize=(20,5))\n",
    "plt.xlabel('Days Employment')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fa4b509",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in day_cols:\n",
    "    data.loc[data[col]< data['DAYS_BIRTH'], col] = data['DAYS_BIRTH']\n",
    "    data.loc[data[col]>0, col] = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a91bee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.loc[[8, 11, 23, 266366],day_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f3e93f8",
   "metadata": {},
   "source": [
    "### Variables qualitatives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0617737c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in [col for col in data.columns if data[col].dtype == 'object']:\n",
    "    print(col,\":\", list(application_train[col].unique()))"
   ]
  },
  {
   "cell_type": "raw",
   "id": "f6e227c3",
   "metadata": {},
   "source": [
    "Il ne nous paraît pas possible dans ce dataset de procéder à une imputation, chaque dossier étant unique. Mais les modèles n'acceptant pas la valeur NaN, si une donnée n'est pas connue, elle sera considérée comme nulle ou vide (selon sa nature) lors de la normalisation.\n",
    "\n",
    "On complète clean_values() :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b3e02f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_values(df):\n",
    "    print('Nettoyage des lignes...')\n",
    "    \n",
    "    old_length = df.shape[0]\n",
    "    result = df.copy()\n",
    "    num_cols = [col for col in result.columns if result[col].dtype!='object']\n",
    "    cat_cols = [col for col in result.columns if col not in num_cols]\n",
    "    \n",
    "    high_z_score = result[num_cols].apply(z_score, axis=0, args=[80])\n",
    "    result.drop(high_z_score[high_z_score.any(axis=1)].index, inplace=True)\n",
    "    \n",
    "    days_cols = [col for col in result.columns if \"DAYS_\" in col]\n",
    "    for col in day_cols:\n",
    "        result.loc[result[col]< result['DAYS_BIRTH'], col] = result['DAYS_BIRTH']\n",
    "        result.loc[result[col]>0, col] = 0\n",
    "        \n",
    "    result[num_cols]=result[num_cols].fillna(0)\n",
    "    result[cat_cols]=result[cat_cols].fillna('Not specified')\n",
    "    \n",
    "    new_length = result.shape[0]\n",
    "    deleted_rows = old_length - new_length\n",
    "    print(f'{deleted_rows} lignes ont été supprimées soit {deleted_rows/old_length:.2%} du total.')\n",
    "  \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f1bd9db",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b4d2fe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_stat(data, feature, title) : \n",
    "    \n",
    "    ax, fig = plt.subplots(figsize=(20,8)) \n",
    "    ax = sns.countplot(y=feature, data=data, order=data[feature].value_counts(ascending=False).index)\n",
    "    ax.set_title(title)\n",
    "\n",
    "    for p in ax.patches:\n",
    "                percentage = '{:.1f}%'.format(100 * p.get_width()/len(data[feature]))\n",
    "                x = p.get_x() + p.get_width()\n",
    "                y = p.get_y() + p.get_height()/2\n",
    "                ax.annotate(percentage, (x, y), fontsize=20, fontweight='bold')\n",
    "\n",
    "def plot_percent_target1(data, feature, title) : \n",
    "    \n",
    "    cat_perc = data[[feature, 'TARGET']].groupby([feature],as_index=False).mean()\n",
    "    cat_perc.sort_values(by='TARGET', ascending=False, inplace=True)\n",
    "    \n",
    "    ax, fig = plt.subplots(figsize=(20,8)) \n",
    "    ax = sns.barplot(y=feature, x='TARGET', data=cat_perc)\n",
    "    ax.set_title(title)\n",
    "    ax.set_xlabel(\"\")\n",
    "    ax.set_ylabel(\"Percent of target with value 1\")\n",
    "\n",
    "    for p in ax.patches:\n",
    "                percentage = '{:.1f}%'.format(100 * p.get_width())\n",
    "                x = p.get_x() + p.get_width()\n",
    "                y = p.get_y() + p.get_height()/2\n",
    "                ax.annotate(percentage, (x, y), fontsize=20, fontweight='bold')\n",
    "\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "202fd5a5",
   "metadata": {},
   "source": [
    "Loan types - Distribution du type de prêts contractés + comparatif avec le pourcentage des prêts avec la valeur TARGET 1(prêt non retourné).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea7b25cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#NAME_CONTRACT_TYPE\n",
    "plot_stat(application_train, 'NAME_CONTRACT_TYPE',\"Type of contract\")\n",
    "print(\"                                   -------------------------------------------------------\")\n",
    "plot_percent_target1(application_train, 'NAME_CONTRACT_TYPE',\"Type of contract %Target1\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "5ba3eda7",
   "metadata": {},
   "source": [
    "Les prêts renouvelables ne représentent qu'une petite fraction (10%) du nombre total de prêts; dans le même temps, un plus grand nombre de crédits renouvelables, par rapport à leur fréquence, ne sont pas remboursés.\n",
    "\n",
    "Client gender - Distribution H/F clients, mais aussi le pourcentage des prêts (par sexe du client) avec la valeur TARGET 1 (prêt non retourné).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "349540be",
   "metadata": {},
   "outputs": [],
   "source": [
    "#CODE_GENDER\n",
    "plot_stat(application_train, 'CODE_GENDER',\"GENDER Distribution\")\n",
    "print(\"                                   -------------------------------------------------------\")\n",
    "plot_percent_target1(application_train, 'CODE_GENDER',\"GENDER Distribution %Target1\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "2c970d0f",
   "metadata": {},
   "source": [
    "Le nombre de clients féminins est presque le double du nombre de clients masculins. En ce qui concerne le pourcentage de crédits en souffrance, les hommes ont plus de chances de ne pas rembourser leurs prêts (10%), comparativement aux femmes (7%).\n",
    "\n",
    "Flag own car - Distribution d'un impact possible entre les clients propriétaire d'un véhicule et ceux qui ne le sont pas…\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3de3f601",
   "metadata": {},
   "outputs": [],
   "source": [
    "#FLAG_OWN_CAR\n",
    "plot_stat(application_train, 'FLAG_OWN_CAR',\"Car owner\")\n",
    "print(\"                                   -------------------------------------------------------\")\n",
    "plot_percent_target1(application_train, 'FLAG_OWN_CAR',\"Car owner %Target1\")\n"
   ]
  },
  {
   "cell_type": "raw",
   "id": "093a1568",
   "metadata": {},
   "source": [
    "Les deux catégories (propriétaire ou non) ont des taux de non-remboursement d'environ 8%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6251352",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAME_FAMILY_STATUS\n",
    "plot_stat(application_train, 'NAME_FAMILY_STATUS',\"Family status\")\n",
    "print(\"                                   -------------------------------------------------------\")\n",
    "plot_percent_target1(application_train, 'NAME_FAMILY_STATUS',\"Family status %Target1\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "ff46cfee",
   "metadata": {},
   "source": [
    "La plupart des clients sont mariés, suivis des célibataires / non mariés et des mariages civils.\n",
    "\n",
    "En termes de pourcentage de non-remboursement du prêt, le mariage civil a le pourcentage le plus élevé de non-remboursement (10%), la veuve étant le plus bas (à l'exception de l'inconnu).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e62a6511",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAME_INCOME_TYPE\n",
    "plot_stat(application_train, 'NAME_INCOME_TYPE',\"Income type of client\")\n",
    "print(\"                                   -------------------------------------------------------\")\n",
    "plot_percent_target1(application_train, 'NAME_INCOME_TYPE',\"Income type of client %Target1\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "fce24ca5",
   "metadata": {},
   "source": [
    "La plupart des demandeurs de prêts sont des revenus du travail, suivis par un associé commercial, un retraité et un fonctionnaire.\n",
    "\n",
    "Les demandeurs avec le type de revenu Congé de maternité ont un ratio de près de 40% de prêts non remboursés, suivis des chômeurs (37%). Les autres types de revenus sont inférieurs à la moyenne de 10% pour ne pas rembourser les prêts.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22e59952",
   "metadata": {},
   "outputs": [],
   "source": [
    "#OCCUPATION_TYPE\n",
    "plot_stat(application_train, 'OCCUPATION_TYPE',\"Ocupation of client\")\n",
    "print(\"                                   -------------------------------------------------------\")\n",
    "plot_percent_target1(application_train, 'OCCUPATION_TYPE',\"Ocupation of client %Target1\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "180592d4",
   "metadata": {},
   "source": [
    "La plupart des prêts sont contractés par des ouvriers, suivis par les vendeurs/commerciaux. Le personnel informatique prend le montant de prêts le plus bas.\n",
    "\n",
    "La catégorie avec le pourcentage le plus élevé de prêts non remboursés est celle des ouvriers peu qualifiés (plus de 17%), suivis des chauffeurs et des serveurs / barmen, du personnel de sécurité, des ouvriers et du personnel de cuisine.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5bc8859",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAME_EDUCATION_TYPE\n",
    "plot_stat(application_train, 'NAME_EDUCATION_TYPE',\"Education type of the client\")\n",
    "print(\"                                   -------------------------------------------------------\")\n",
    "plot_percent_target1(application_train, 'NAME_EDUCATION_TYPE',\"Education type of the client %Target1\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4713907f",
   "metadata": {},
   "source": [
    "La majorité des clients ont une éducation dans l'éducation secondaire, suivis des clients avec une éducation supérieure. Un très petit nombre d'emprunteur possède un diplôme universitaire.\n",
    "\n",
    "La catégorie du premier cycle du secondaire, bien que rare, a le taux le plus élevé de non-remboursement du prêt (11%). Les personnes ayant un diplôme universitaire ont un taux de non-remboursement inférieur à 2%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a9effa3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#NAME_HOUSING_TYPE\n",
    "plot_stat(application_train, 'NAME_HOUSING_TYPE',\"Type of the housing of client\")\n",
    "print(\"                                   -------------------------------------------------------\")\n",
    "plot_percent_target1(application_train, 'NAME_HOUSING_TYPE',\"Type of the housing of client %Target1\")"
   ]
  },
  {
   "cell_type": "raw",
   "id": "4ccc8c44",
   "metadata": {},
   "source": [
    "Plus de 250 000 demandeurs de crédits vivent en maison ou appartement. Les catégories suivantes, faible pourcentage, représentent une population moins \"indépendante\" (vivre chez ses parents, etc…).\n",
    "\n",
    "Dans ces catégories, les loueurs d'appartements (non propriétaires de leur résidence principale), ainsi que ceux qui vivent chez leurs parents, ont un taux de non-remboursement supérieur à 10%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "194b609a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11fc9c14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#kernel lightgbm\n",
    "# taux d'acceptation ou taux de refus trop elevé \n",
    "# matrice de confusion\n",
    "# python anywhere ou aws\n",
    "#stream light\n",
    "# frontend backend\n",
    "#flask ou django\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd49ec1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import gc\n",
    "import time\n",
    "from contextlib import contextmanager\n",
    "from lightgbm import LGBMClassifier\n",
    "from sklearn.metrics import roc_auc_score, roc_curve\n",
    "from sklearn.model_selection import KFold, StratifiedKFold\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import warnings\n",
    "#pip install lightgbm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f943ae25",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.simplefilter(action='ignore', category=FutureWarning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1a6c7f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''#dirpath = \"C:/Users/hlouk/OpenClassroom/P7/\"\n",
    "dirpath = \"C:/Users/Allihamdulilahi/Desktop/invest_ideas/han/p7/\"\n",
    "\n",
    "application_train = pd.read_csv(dirpath+\"application_train.csv\")\n",
    "application_test = pd.read_csv(dirpath+\"application_test.csv\")\n",
    "bureau = pd.read_csv(dirpath+\"bureau.csv\")\n",
    "bureau_balance = pd.read_csv(dirpath+\"bureau_balance.csv\")\n",
    "cash_balance = pd.read_csv(dirpath+\"POS_CASH_balance.csv\")\n",
    "card_balance = pd.read_csv(dirpath+\"credit_card_balance.csv\")\n",
    "prev_app = pd.read_csv(dirpath+\"previous_application.csv\")\n",
    "payments = pd.read_csv(dirpath+\"installments_payments.csv\")\n",
    "HomeCredit = pd.read_csv(dirpath+\"HomeCredit_columns_description.csv\",  na_values = missing_values, encoding='latin1')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4c855273",
   "metadata": {},
   "outputs": [],
   "source": [
    "@contextmanager\n",
    "def timer(title):\n",
    "    t0 = time.time()\n",
    "    yield\n",
    "    print(\"{} - done in {:.0f}s\".format(title, time.time() - t0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0d13ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# One-hot encoding for categorical columns with get_dummies\n",
    "def one_hot_encoder(df, nan_as_category = True):\n",
    "    original_columns = list(df.columns)\n",
    "    categorical_columns = [col for col in df.columns if df[col].dtype == 'object']\n",
    "    df = pd.get_dummies(df, columns= categorical_columns, dummy_na= nan_as_category)\n",
    "    new_columns = [c for c in df.columns if c not in original_columns]\n",
    "    return df, new_columns\n",
    "\n",
    "# Preprocess application_train.csv and application_test.csv\n",
    "def application_train_test(num_rows = None, nan_as_category = False):\n",
    "    # Read data and merge\n",
    "    df = pd.read_csv('../p7/application_train.csv', nrows= num_rows)\n",
    "    test_df = pd.read_csv('../p7/application_test.csv', nrows= num_rows)\n",
    "    print(\"Train samples: {}, test samples: {}\".format(len(df), len(test_df)))\n",
    "    df = df.append(test_df).reset_index()\n",
    "    # Optional: Remove 4 applications with XNA CODE_GENDER (train set)\n",
    "    df = df[df['CODE_GENDER'] != 'XNA']\n",
    "    \n",
    "    # Categorical features with Binary encode (0 or 1; two categories)\n",
    "    for bin_feature in ['CODE_GENDER', 'FLAG_OWN_CAR', 'FLAG_OWN_REALTY']:\n",
    "        df[bin_feature], uniques = pd.factorize(df[bin_feature])\n",
    "    # Categorical features with One-Hot encode\n",
    "    df, cat_cols = one_hot_encoder(df, nan_as_category)\n",
    "    \n",
    "    # NaN values for DAYS_EMPLOYED: 365.243 -> nan\n",
    "    df['DAYS_EMPLOYED'].replace(365243, np.nan, inplace= True)\n",
    "    # Some simple new features (percentages)\n",
    "    df['DAYS_EMPLOYED_PERC'] = df['DAYS_EMPLOYED'] / df['DAYS_BIRTH']\n",
    "    df['INCOME_CREDIT_PERC'] = df['AMT_INCOME_TOTAL'] / df['AMT_CREDIT']\n",
    "    df['INCOME_PER_PERSON'] = df['AMT_INCOME_TOTAL'] / df['CNT_FAM_MEMBERS']\n",
    "    df['ANNUITY_INCOME_PERC'] = df['AMT_ANNUITY'] / df['AMT_INCOME_TOTAL']\n",
    "    df['PAYMENT_RATE'] = df['AMT_ANNUITY'] / df['AMT_CREDIT']\n",
    "    del test_df\n",
    "    gc.collect()\n",
    "    return df\n",
    "\n",
    "# Preprocess bureau.csv and bureau_balance.csv\n",
    "def bureau_and_balance(num_rows = None, nan_as_category = True):\n",
    "    bureau = pd.read_csv('../p7/bureau.csv', nrows = num_rows)\n",
    "    bb = pd.read_csv('../p7/bureau_balance.csv', nrows = num_rows)\n",
    "    bb, bb_cat = one_hot_encoder(bb, nan_as_category)\n",
    "    bureau, bureau_cat = one_hot_encoder(bureau, nan_as_category)\n",
    "    \n",
    "    # Bureau balance: Perform aggregations and merge with bureau.csv\n",
    "    bb_aggregations = {'MONTHS_BALANCE': ['min', 'max', 'size']}\n",
    "    for col in bb_cat:\n",
    "        bb_aggregations[col] = ['mean']\n",
    "    bb_agg = bb.groupby('SK_ID_BUREAU').agg(bb_aggregations)\n",
    "    bb_agg.columns = pd.Index([e[0] + \"_\" + e[1].upper() for e in bb_agg.columns.tolist()])\n",
    "    bureau = bureau.join(bb_agg, how='left', on='SK_ID_BUREAU')\n",
    "    bureau.drop(['SK_ID_BUREAU'], axis=1, inplace= True)\n",
    "    del bb, bb_agg\n",
    "    gc.collect()\n",
    "    \n",
    "    # Bureau and bureau_balance numeric features\n",
    "    num_aggregations = {\n",
    "        'DAYS_CREDIT': ['min', 'max', 'mean', 'var'],\n",
    "        'DAYS_CREDIT_ENDDATE': ['min', 'max', 'mean'],\n",
    "        'DAYS_CREDIT_UPDATE': ['mean'],\n",
    "        'CREDIT_DAY_OVERDUE': ['max', 'mean'],\n",
    "        'AMT_CREDIT_MAX_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_DEBT': ['max', 'mean', 'sum'],\n",
    "        'AMT_CREDIT_SUM_OVERDUE': ['mean'],\n",
    "        'AMT_CREDIT_SUM_LIMIT': ['mean', 'sum'],\n",
    "        'AMT_ANNUITY': ['max', 'mean'],\n",
    "        'CNT_CREDIT_PROLONG': ['sum'],\n",
    "        'MONTHS_BALANCE_MIN': ['min'],\n",
    "        'MONTHS_BALANCE_MAX': ['max'],\n",
    "        'MONTHS_BALANCE_SIZE': ['mean', 'sum']\n",
    "    }\n",
    "    # Bureau and bureau_balance categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in bureau_cat: cat_aggregations[cat] = ['mean']\n",
    "    for cat in bb_cat: cat_aggregations[cat + \"_MEAN\"] = ['mean']\n",
    "    \n",
    "    bureau_agg = bureau.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    bureau_agg.columns = pd.Index(['BURO_' + e[0] + \"_\" + e[1].upper() for e in bureau_agg.columns.tolist()])\n",
    "    # Bureau: Active credits - using only numerical aggregations\n",
    "    active = bureau[bureau['CREDIT_ACTIVE_Active'] == 1]\n",
    "    active_agg = active.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    active_agg.columns = pd.Index(['ACTIVE_' + e[0] + \"_\" + e[1].upper() for e in active_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(active_agg, how='left', on='SK_ID_CURR')\n",
    "    del active, active_agg\n",
    "    gc.collect()\n",
    "    # Bureau: Closed credits - using only numerical aggregations\n",
    "    closed = bureau[bureau['CREDIT_ACTIVE_Closed'] == 1]\n",
    "    closed_agg = closed.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    closed_agg.columns = pd.Index(['CLOSED_' + e[0] + \"_\" + e[1].upper() for e in closed_agg.columns.tolist()])\n",
    "    bureau_agg = bureau_agg.join(closed_agg, how='left', on='SK_ID_CURR')\n",
    "    del closed, closed_agg, bureau\n",
    "    gc.collect()\n",
    "    return bureau_agg\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9b3b4850",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Preprocess previous_applications.csv\n",
    "def previous_applications(num_rows = None, nan_as_category = True):\n",
    "    prev = pd.read_csv('../p7/previous_application.csv', nrows = num_rows)\n",
    "    prev, cat_cols = one_hot_encoder(prev, nan_as_category= True)\n",
    "    # Days 365.243 values -> nan\n",
    "    prev['DAYS_FIRST_DRAWING'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_FIRST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE_1ST_VERSION'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_LAST_DUE'].replace(365243, np.nan, inplace= True)\n",
    "    prev['DAYS_TERMINATION'].replace(365243, np.nan, inplace= True)\n",
    "    # Add feature: value ask / value received percentage\n",
    "    prev['APP_CREDIT_PERC'] = prev['AMT_APPLICATION'] / prev['AMT_CREDIT']\n",
    "    # Previous applications numeric features\n",
    "    num_aggregations = {\n",
    "        'AMT_ANNUITY': ['min', 'max', 'mean'],\n",
    "        'AMT_APPLICATION': ['min', 'max', 'mean'],\n",
    "        'AMT_CREDIT': ['min', 'max', 'mean'],\n",
    "        'APP_CREDIT_PERC': ['min', 'max', 'mean', 'var'],\n",
    "        'AMT_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'AMT_GOODS_PRICE': ['min', 'max', 'mean'],\n",
    "        'HOUR_APPR_PROCESS_START': ['min', 'max', 'mean'],\n",
    "        'RATE_DOWN_PAYMENT': ['min', 'max', 'mean'],\n",
    "        'DAYS_DECISION': ['min', 'max', 'mean'],\n",
    "        'CNT_PAYMENT': ['mean', 'sum'],\n",
    "    }\n",
    "    # Previous applications categorical features\n",
    "    cat_aggregations = {}\n",
    "    for cat in cat_cols:\n",
    "        cat_aggregations[cat] = ['mean']\n",
    "    \n",
    "    prev_agg = prev.groupby('SK_ID_CURR').agg({**num_aggregations, **cat_aggregations})\n",
    "    prev_agg.columns = pd.Index(['PREV_' + e[0] + \"_\" + e[1].upper() for e in prev_agg.columns.tolist()])\n",
    "    # Previous Applications: Approved Applications - only numerical features\n",
    "    approved = prev[prev['NAME_CONTRACT_STATUS_Approved'] == 1]\n",
    "    approved_agg = approved.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    approved_agg.columns = pd.Index(['APPROVED_' + e[0] + \"_\" + e[1].upper() for e in approved_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(approved_agg, how='left', on='SK_ID_CURR')\n",
    "    # Previous Applications: Refused Applications - only numerical features\n",
    "    refused = prev[prev['NAME_CONTRACT_STATUS_Refused'] == 1]\n",
    "    refused_agg = refused.groupby('SK_ID_CURR').agg(num_aggregations)\n",
    "    refused_agg.columns = pd.Index(['REFUSED_' + e[0] + \"_\" + e[1].upper() for e in refused_agg.columns.tolist()])\n",
    "    prev_agg = prev_agg.join(refused_agg, how='left', on='SK_ID_CURR')\n",
    "    del refused, refused_agg, approved, approved_agg, prev\n",
    "    gc.collect()\n",
    "    return prev_agg\n",
    "\n",
    "# Preprocess POS_CASH_balance.csv\n",
    "def pos_cash(num_rows = None, nan_as_category = True):\n",
    "    pos = pd.read_csv('../p7/POS_CASH_balance.csv', nrows = num_rows)\n",
    "    pos, cat_cols = one_hot_encoder(pos, nan_as_category= True)\n",
    "    # Features\n",
    "    aggregations = {\n",
    "        'MONTHS_BALANCE': ['max', 'mean', 'size'],\n",
    "        'SK_DPD': ['max', 'mean'],\n",
    "        'SK_DPD_DEF': ['max', 'mean']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    \n",
    "    pos_agg = pos.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    pos_agg.columns = pd.Index(['POS_' + e[0] + \"_\" + e[1].upper() for e in pos_agg.columns.tolist()])\n",
    "    # Count pos cash accounts\n",
    "    pos_agg['POS_COUNT'] = pos.groupby('SK_ID_CURR').size()\n",
    "    del pos\n",
    "    gc.collect()\n",
    "    return pos_agg\n",
    "    \n",
    "# Preprocess installments_payments.csv\n",
    "def installments_payments(num_rows = None, nan_as_category = True):\n",
    "    ins = pd.read_csv('../p7/installments_payments.csv', nrows = num_rows)\n",
    "    ins, cat_cols = one_hot_encoder(ins, nan_as_category= True)\n",
    "    # Percentage and difference paid in each installment (amount paid and installment value)\n",
    "    ins['PAYMENT_PERC'] = ins['AMT_PAYMENT'] / ins['AMT_INSTALMENT']\n",
    "    ins['PAYMENT_DIFF'] = ins['AMT_INSTALMENT'] - ins['AMT_PAYMENT']\n",
    "    # Days past due and days before due (no negative values)\n",
    "    ins['DPD'] = ins['DAYS_ENTRY_PAYMENT'] - ins['DAYS_INSTALMENT']\n",
    "    ins['DBD'] = ins['DAYS_INSTALMENT'] - ins['DAYS_ENTRY_PAYMENT']\n",
    "    ins['DPD'] = ins['DPD'].apply(lambda x: x if x > 0 else 0)\n",
    "    ins['DBD'] = ins['DBD'].apply(lambda x: x if x > 0 else 0)\n",
    "    # Features: Perform aggregations\n",
    "    aggregations = {\n",
    "        'NUM_INSTALMENT_VERSION': ['nunique'],\n",
    "        'DPD': ['max', 'mean', 'sum'],\n",
    "        'DBD': ['max', 'mean', 'sum'],\n",
    "        'PAYMENT_PERC': ['max', 'mean', 'sum', 'var'],\n",
    "        'PAYMENT_DIFF': ['max', 'mean', 'sum', 'var'],\n",
    "        'AMT_INSTALMENT': ['max', 'mean', 'sum'],\n",
    "        'AMT_PAYMENT': ['min', 'max', 'mean', 'sum'],\n",
    "        'DAYS_ENTRY_PAYMENT': ['max', 'mean', 'sum']\n",
    "    }\n",
    "    for cat in cat_cols:\n",
    "        aggregations[cat] = ['mean']\n",
    "    ins_agg = ins.groupby('SK_ID_CURR').agg(aggregations)\n",
    "    ins_agg.columns = pd.Index(['INSTAL_' + e[0] + \"_\" + e[1].upper() for e in ins_agg.columns.tolist()])\n",
    "    # Count installments accounts\n",
    "    ins_agg['INSTAL_COUNT'] = ins.groupby('SK_ID_CURR').size()\n",
    "    del ins\n",
    "    gc.collect()\n",
    "    return ins_agg\n",
    "\n",
    "# Preprocess credit_card_balance.csv\n",
    "def credit_card_balance(num_rows = None, nan_as_category = True):\n",
    "    cc = pd.read_csv('../p7/credit_card_balance.csv', nrows = num_rows)\n",
    "    cc, cat_cols = one_hot_encoder(cc, nan_as_category= True)\n",
    "    # General aggregations\n",
    "    cc.drop(['SK_ID_PREV'], axis= 1, inplace = True)\n",
    "    cc_agg = cc.groupby('SK_ID_CURR').agg(['min', 'max', 'mean', 'sum', 'var'])\n",
    "    cc_agg.columns = pd.Index(['CC_' + e[0] + \"_\" + e[1].upper() for e in cc_agg.columns.tolist()])\n",
    "    # Count credit card lines\n",
    "    cc_agg['CC_COUNT'] = cc.groupby('SK_ID_CURR').size()\n",
    "    del cc\n",
    "    gc.collect()\n",
    "    return cc_agg\n",
    "\n",
    "# LightGBM GBDT with KFold or Stratified KFold\n",
    "# Parameters from Tilii kernel: https://www.kaggle.com/tilii7/olivier-lightgbm-parameters-by-bayesian-opt/code\n",
    "def kfold_lightgbm(df, num_folds, stratified = False, debug= False):\n",
    "    # Divide in training/validation and test data\n",
    "    train_df = df[df['TARGET'].notnull()]\n",
    "    test_df = df[df['TARGET'].isnull()]\n",
    "    train_df = train_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    test_df = test_df.rename(columns = lambda x:re.sub('[^A-Za-z0-9_]+', '', x))\n",
    "    print(\"Starting LightGBM. Train shape: {}, test shape: {}\".format(train_df.shape, test_df.shape))\n",
    "    del df\n",
    "    gc.collect()\n",
    "    # Cross validation model\n",
    "    if stratified:\n",
    "        folds = StratifiedKFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    else:\n",
    "        folds = KFold(n_splits= num_folds, shuffle=True, random_state=1001)\n",
    "    # Create arrays and dataframes to store results\n",
    "    oof_preds = np.zeros(train_df.shape[0])\n",
    "    sub_preds = np.zeros(test_df.shape[0])\n",
    "    feature_importance_df = pd.DataFrame()\n",
    "    feats = [f for f in train_df.columns if f not in ['TARGET','SK_ID_CURR','SK_ID_BUREAU','SK_ID_PREV','index']]\n",
    "    \n",
    "    for n_fold, (train_idx, valid_idx) in enumerate(folds.split(train_df[feats], train_df['TARGET'])):\n",
    "        train_x, train_y = train_df[feats].iloc[train_idx], train_df['TARGET'].iloc[train_idx]\n",
    "        valid_x, valid_y = train_df[feats].iloc[valid_idx], train_df['TARGET'].iloc[valid_idx]\n",
    "\n",
    "        # LightGBM parameters found by Bayesian optimization\n",
    "        clf = LGBMClassifier(\n",
    "            nthread=4,\n",
    "            n_estimators=10000,\n",
    "            learning_rate=0.02,\n",
    "            num_leaves=34,\n",
    "            colsample_bytree=0.9497036,\n",
    "            subsample=0.8715623,\n",
    "            max_depth=8,\n",
    "            reg_alpha=0.041545473,\n",
    "            reg_lambda=0.0735294,\n",
    "            min_split_gain=0.0222415,\n",
    "            min_child_weight=39.3259775,\n",
    "            silent=-1,\n",
    "            verbose=-1, )\n",
    "\n",
    "        clf.fit(train_x, train_y, eval_set=[(train_x, train_y), (valid_x, valid_y)], \n",
    "            eval_metric= 'auc', verbose= 200, early_stopping_rounds= 200)\n",
    "\n",
    "        oof_preds[valid_idx] = clf.predict_proba(valid_x, num_iteration=clf.best_iteration_)[:, 1]\n",
    "        sub_preds += clf.predict_proba(test_df[feats], num_iteration=clf.best_iteration_)[:, 1] / folds.n_splits\n",
    "\n",
    "        fold_importance_df = pd.DataFrame()\n",
    "        fold_importance_df[\"feature\"] = feats\n",
    "        fold_importance_df[\"importance\"] = clf.feature_importances_\n",
    "        fold_importance_df[\"fold\"] = n_fold + 1\n",
    "        feature_importance_df = pd.concat([feature_importance_df, fold_importance_df], axis=0)\n",
    "        print('Fold %2d AUC : %.6f' % (n_fold + 1, roc_auc_score(valid_y, oof_preds[valid_idx])))\n",
    "        del clf, train_x, train_y, valid_x, valid_y\n",
    "        gc.collect()\n",
    "\n",
    "    print('Full AUC score %.6f' % roc_auc_score(train_df['TARGET'], oof_preds))\n",
    "    # Write submission file and plot feature importance\n",
    "    if not debug:\n",
    "        test_df['TARGET'] = sub_preds\n",
    "        test_df[['SK_ID_CURR', 'TARGET']].to_csv(submission_file_name, index= False)\n",
    "    display_importances(feature_importance_df)\n",
    "    return feature_importance_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6155bdd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#No such file or directory: '../input/application_train.csv'\n",
    "# Display/plot feature importance\n",
    "import re\n",
    "\n",
    "def display_importances(feature_importance_df_):\n",
    "    cols = feature_importance_df_[[\"feature\", \"importance\"]].groupby(\"feature\").mean().sort_values(by=\"importance\", ascending=False)[:40].index\n",
    "    best_features = feature_importance_df_.loc[feature_importance_df_.feature.isin(cols)]\n",
    "    plt.figure(figsize=(8, 10))\n",
    "    sns.barplot(x=\"importance\", y=\"feature\", data=best_features.sort_values(by=\"importance\", ascending=False))\n",
    "    plt.title('LightGBM Features (avg over folds)')\n",
    "    plt.tight_layout()\n",
    "    plt.savefig('lgbm_importances01.png')\n",
    "\n",
    "\n",
    "def main(debug = False):\n",
    "    num_rows = 10000 if debug else None\n",
    "    df = application_train_test(num_rows)\n",
    "    with timer(\"Process bureau and bureau_balance\"):\n",
    "        bureau = bureau_and_balance(num_rows)\n",
    "        print(\"Bureau df shape:\", bureau.shape)\n",
    "        df = df.join(bureau, how='left', on='SK_ID_CURR')\n",
    "        del bureau\n",
    "        gc.collect()\n",
    "    with timer(\"Process previous_applications\"):\n",
    "        prev = previous_applications(num_rows)\n",
    "        print(\"Previous applications df shape:\", prev.shape)\n",
    "        df = df.join(prev, how='left', on='SK_ID_CURR')\n",
    "        del prev\n",
    "        gc.collect()\n",
    "    with timer(\"Process POS-CASH balance\"):\n",
    "        pos = pos_cash(num_rows)\n",
    "        print(\"Pos-cash balance df shape:\", pos.shape)\n",
    "        df = df.join(pos, how='left', on='SK_ID_CURR')\n",
    "        del pos\n",
    "        gc.collect()\n",
    "    with timer(\"Process installments payments\"):\n",
    "        ins = installments_payments(num_rows)\n",
    "        print(\"Installments payments df shape:\", ins.shape)\n",
    "        df = df.join(ins, how='left', on='SK_ID_CURR')\n",
    "        del ins\n",
    "        gc.collect()\n",
    "    with timer(\"Process credit card balance\"):\n",
    "        cc = credit_card_balance(num_rows)\n",
    "        print(\"Credit card balance df shape:\", cc.shape)\n",
    "        df = df.join(cc, how='left', on='SK_ID_CURR')\n",
    "        del cc\n",
    "        gc.collect()\n",
    "    with timer(\"Run LightGBM with kfold\"):\n",
    "        feat_importance = kfold_lightgbm(df, num_folds= 10, stratified= False, debug= debug)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    submission_file_name = \"submission_kernel02.csv\"\n",
    "    with timer(\"Full model run\"):\n",
    "        main()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e3bf0550",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "306.183px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
